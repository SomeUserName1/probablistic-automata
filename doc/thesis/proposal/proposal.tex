\documentclass[11pt, rgb]{scrartcl}
\usepackage{themeKonstanz} % Muss immer verwendet werden (Standardpaket)
\format{a4}

% Thesis information        %
\date{\today}
\year{2020}
\author{Fabian Klopfer}

\title{Minimization of Stochastic Dynamical Systems}
\subtitle{A Comparison of Models and Methods}
\unisection{Faculty of Sciences}
\department{Department of Computer and Information Science}
\group{Modelling of Complex Self-Organizing Systems Group}
\supervisorOne{Prof. Dr. Tatjana Petrov}

\bibliography{resources} 

\begin{document}

\newgeometry{left=2.5cm, right=2.5cm, bottom=2cm, top=2.5cm, headheight=12pt, headsep=0.8cm, footskip=22pt}
\thesistitlepage[language=english]{M.Sc. Thesis Proposal, Draft v0.1}
\restoregeometry
\textit{@Tanja:
\begin{itemize}
 \item What would be a more suitable title?
  \item Are introduction and problem statement okay like that?
 \item Wrt. Methods and literature: Is it okay to describe the literature you showed, explained and sent to me or should I look for/add additional relevant literature?
 \item shall I add something on how the presentations and meetings are planned? I wouldnt add a roadmap as last time as my roadmap was completely off and overly optimistic
 \item Comments/Feedback?
\end{itemize}
}

\section{Introduction} In the course of the project, two different approaches for minimizing dynamical systems in different modelling frameworks were considered: \\

On the one hand a method by Kiefer et al.~\autocite{Kiefer2013OnTC}, that uses the framework of weighted automata and is based on the work by Sch√ºtzenberger\autocite{schutz}. 
It constructs the space of all words that can be reached from the starting state in $n$ steps where $n$ is the number of states. 
Those vectors are generated by multiplying the initial vector with the transition matrices corresponding to the considered word.
Then a random vector of size $n \times |\text{Alphabet}|$ is generated and each word vector is multiplied by a random factor that is determined from the random vector and the word structure. The word vectors multiplied by the factors are then added up.
This is done $n$ times, each of these so generated $n$ vectors is stacked as row vector into a matrix.
Finally the initial vector is stacked atop of this matrix forming a $n+1 \times n$ matrix.
To complete the construction the maximally linearly independent subset of rows is chosen as basis of the prefix space.
Then Schuetzenberger construction is applied using this matrix. Schuetzenberger construction uses the just constructed basis to transfrom the transition matrix, the initial and the final vector to a new lower-dimensional representation following the new basis. 
Thus the matrix constructed is some sort of projection from $n$ to $m$ dimensions with included reweighting and is thus a bisimulation following the definitions by Buchholz~\autocite{buchholz2008bisimulation}.
The method of Kiefer continues by applying the same steps using the postfix space instead of the prefix space. \\
This procedure has a worst case complexity that is exponential as the powerset of words with up to $n$ letters has to be constructed: $|\text{Alphabet}|^n$. If we are assuming a sparse prefix space, that is only a few of all possible words are a possible prefix of the language accepted by the automaton, we get the average runtime. As Schuetzenberger construction uses matrix multiplication, which is in $\mathcal{O}(n^3)$ the algorithm also has an asymptotic average runtime of $\mathcal{O}(n^3)$. The algorithm is guarenteed to yield a minimal representation with respect to the number of states but produces often more non zero entries in the transition matrix. \\

On the other hand, a method by Tribastone et al.\autocite{Cardelli2017MaximalAO}, that builds upon the algorithm of Valmari for Markov Chains\autocite{valmari} and thus Paigeand Tarjan\autocite{paigetarjan}, extending it to the framework of chemical reaction networks (CRNs) or to be formally more strictly abstract rewrite systems. The algorithm is actually targeted at polynomial ordinary differential equations (ODEs), but the authors provide a conversion from ODEs to CRNS. The algorithm then proceeds by calculating a quantity for each species and possible left-handside of the rule set and use this to split the species into partitions using the standard partition refinememnt approach that corresponds closely to~\autocite{valmari}. The quantitiy to compute is the contribution by the authors and comes in two flavors: Forward differential equivalence uses stoichiometry per partition to find species that behave the same when ``reacting'' to other species, while backward differential equivalence tries to find species that behave the same with respect to the reagents they are ``reacted'' from. According to the authors this shall be in correspondence with ordinary lumpability for forward differential equivalence (FDE) and exact lumpability for backward differential equivalence (BDE). While the first correspondence seems to be quite close, the latter assumes uniform initial values which is an aspect that weak lumpability handles, thus BDE seems to be closer related to weak lumpability. To state it in Buchholz's terms~\autocite{buchholz}:
\begin{quote}
 For exactly lumpable partitions exact results for individual probabilities of the original Markov chain can be computed for a special class of initial vectors, namely for all vectors for which the partition is weakly lumpable.
\end{quote}
The procedure enjoys linearithmic average runtime complexity, resulting from using the partition refinement schema by Valmari et al~\autocite{valmari}. This approach does not guarantee minimality, which is already shown for BDE by Boreale~\autocite{boreale2017algebra}. \\
 

\section{Problem Statement and Objectives of Study}
The two methods above use different approaches to induce bisimulation relations: The first method uses linear algebra to find a lower dimensional basis, while the second method uses a stoichiometry-based quantity to find the coarsest partition with respect to the quantity. \\
This brings up the question weather the two are equivalent, thus yield the same representation with the same degree of reducetion when the same dynamic system is input in the corresponding representation in the other framework. Do both compute the same kind of bisimulations and what are the guarantees that the two provide in terms of reduction. \\

In order to address this questions, one needs to convert a weighted automaton into a CRN and vice versa. To do this it seems most appropriate to approach the common ground of both methods: Probabilistic automata, whereof Markov chains are a subset with special semantics. The method of Tribastone et al.~\autocite{Cardelli2017MaximalAO} is derived from Markov chains using the notions of lumping, the method of Kiefer addresses a superset of probabilistic automata, that is weighted automata. Essentially this boils the task down to find a conversion from CRNs to probabilistic automata and from weighted automata to probabilisitc automata. Is this possible? Does this preserve important semantic properties like the language accepted by the automata, the execution semantics of both models and is it possible to assign a parametrized language to a CRN when adding a parametrized initial and boundary condition the other way round --- when leaving the initial and final vector unbound as a parameter, is it possible to derive some rewriting system that induces a grammar when bound with concrete values? Is there some trick that allows to circumvent this parametrization e.g. by using the one vector as an initial vector or an auxilliary state and letter int the alphabet that with a parametrized transition matrix? \\

In the end, questions on which framework is more flexible, expressive, succint, has better computational properties and covers certains language theoretic aspects arise. Similar ones arise for the reduction methods that are applicable and certain variants of them. If we find a method in one domain, is it straight forward to port it into the other?


\section{Methods and Literature}
TODO: Papers send by tanja, fully probabilistic Segala, Picture WA, PA, MC ,CRN, ODE





\printbibliography

\end{document}
